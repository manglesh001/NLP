{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7921409,"sourceType":"datasetVersion","datasetId":4655095},{"sourceId":7930308,"sourceType":"datasetVersion","datasetId":4661225},{"sourceId":7986652,"sourceType":"datasetVersion","datasetId":4701307}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Set display options to show full text in each cell\npd.set_option('display.max_colwidth', None)\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/train-csv/train.csv')\n\n# Display top 20 rows\nprint(\"Top 20 rows:\")\nprint(data.head(20))\n\n# Extract 'class_index' and 'description' columns\nclass_index = data['Class Index']\ndescription = data['Description']\n\n# Count unique values in 'class_index'\nunique_class_count = class_index.nunique()\n\nprint(\"\\nNumber of unique values in 'class_index':\", unique_class_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T08:47:09.545457Z","iopub.execute_input":"2024-03-27T08:47:09.545814Z","iopub.status.idle":"2024-03-27T08:47:09.851614Z","shell.execute_reply.started":"2024-03-27T08:47:09.545783Z","shell.execute_reply":"2024-03-27T08:47:09.850714Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Top 20 rows:\n    Class Index  \\\n0             3   \n1             3   \n2             3   \n3             3   \n4             3   \n5             3   \n6             3   \n7             3   \n8             3   \n9             3   \n10            3   \n11            3   \n12            3   \n13            3   \n14            3   \n15            3   \n16            3   \n17            3   \n18            3   \n19            3   \n\n                                                                                                                                                                                                                                                                                                                                                                                               Description  \n0                                                                                                                                                                                                                                                                                                           Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.  \n1                                                                                                                                                                                   Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.  \n2                                                                                                                                                                                                                 Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.  \n3                                                                                                                                                                                                      Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.  \n4                                                                                                                                                                                                                                         AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.  \n5                                                                                                                                                                                                      Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)  \n6                                                                                                                                                                                                                       AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.  \n7                                                                                                                                                                                                    USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.  \n8   Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.  \n9                                                                                                                                                                                                                                                                                               NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.  \n10                                                                                                                                                                                                  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.  \n11                                                                                                                                                                TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when markets are already oversupplied by 2.8 million  barrels per day (bpd) of crude, Iran's OPEC governor said  Saturday, warning that prices could fall sharply.  \n12                                                                                                                                                                                                                                          JAKARTA (Reuters) - Non-OPEC oil exporters should consider  increasing output to cool record crude prices, OPEC President  Purnomo Yusgiantoro said on Sunday.  \n13                                                                                                                                                                       WASHINGTON/NEW YORK (Reuters) - The auction for Google  Inc.'s highly anticipated initial public offering got off to a  rocky start on Friday after the Web search company sidestepped  a bullet from U.S. securities regulators.  \n14                                                                                                                                                                        NEW YORK (Reuters) - The dollar tumbled broadly on Friday  after data showing a record U.S. trade deficit in June cast  fresh doubts on the economy's recovery and its ability to draw  foreign capital to fund the growing gap.  \n15                                                                                                                                                                                                                                                                         If you think you may need to help your elderly relatives with their finances, don't be shy about having the money talk -- soon.  \n16                                                                                                                                                                                                                                                                                The purchasing power of kids is a big part of why the back-to-school season has become such a huge marketing phenomenon.  \n17                                                                                                                                                                             There is little cause for celebration in the stock market these days, but investors in value-focused mutual funds have reason to feel a bit smug -- if only because they've lost less than the folks who stuck with growth.  \n18                                                                                                                                                                                                                                                                             The US trade deficit has exploded 19 to a record \\$55.8bn as oil costs drove imports higher, according to a latest figures.  \n19                                                                                                                                                                                                                                                                               Oil giant Shell could be bracing itself for a takeover attempt, possibly from French rival Total, a  press report claims.  \n\nNumber of unique values in 'class_index': 4\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom scipy.sparse import coo_matrix\nfrom sklearn.decomposition import TruncatedSVD\nfrom collections import defaultdict\nimport string\nimport re\nimport torch\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/train-csv/train.csv')\n\n# Extract descriptions\ndescriptions = df['Description'].iloc[0:15000]\nprint(descriptions.shape)\n# Preprocessing function\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    \n    return tokens\n\n# Build co-occurrence matrix\ndef build_co_occurrence_matrix(descriptions, window_size=1):\n    word_to_id = {}\n    \n    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n    word_freq = defaultdict(int)\n    \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for token in tokens:\n            word_freq[token] += 1\n        \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for i, word in enumerate(tokens):\n            if word_freq[word] < 5:\n#                 word = '<UNK>'\n                tokens[i]=\"<UNK>\"\n            if word not in word_to_id:\n                word_id = len(word_to_id)\n                word_to_id[word] = word_id\n\n            start_index = max(0, i - window_size)\n            end_index = min(len(tokens), i + window_size + 1)\n            context_words = tokens[start_index:i] + tokens[i+1:end_index]\n            \n            for context_word in context_words:\n                if word_freq[context_word] < 5:\n                    context_word = '<UNK>'\n                if context_word not in word_to_id:\n                    word_id = len(word_to_id)\n                    word_to_id[context_word] = word_id\n                co_occurrence_matrix[word_to_id[word]][word_to_id[context_word]] += 1\n    print(len(word_to_id))\n\n    return co_occurrence_matrix, word_to_id\n\n# Convert co-occurrence matrix to a sparse matrix\ndef co_occurrence_to_sparse_matrix(co_occurrence_matrix):\n    rows = []\n    cols = []\n    data = []\n    for i, row in co_occurrence_matrix.items():\n        for j, value in row.items():\n            rows.append(i)\n            cols.append(j)\n            data.append(value)\n    return coo_matrix((data, (rows, cols)))\n\n# Apply Singular Value Decomposition (SVD)\ndef apply_svd(matrix, k=150):\n    svd = TruncatedSVD(n_components=k, random_state=42)\n    word_vectors = svd.fit_transform(matrix)\n    return word_vectors\n\n# Save word vectors\ndef save_word_vectors(word_vectors, word_to_id, filename):\n    with open(filename, 'w') as f:\n        for word, word_id in word_to_id.items():\n            vector_str = ' '.join(map(str, word_vectors[word_id]))\n            f.write(f'{word} {vector_str}\\n')\n\n# Build co-occurrence matrix\nco_occurrence_matrix, word_to_id = build_co_occurrence_matrix(descriptions)\n\n# Convert co-occurrence matrix to sparse matrix\nsparse_matrix = co_occurrence_to_sparse_matrix(co_occurrence_matrix)\n\n# Apply SVD\nword_vectors = apply_svd(sparse_matrix)\n\n# Save word vectors\n\nprint(word_vectors[0])\n\nmodel_save_name = 'svd-word-vectors_15k.pt'\npath = F\"/kaggle/working/{model_save_name}\"\ntorch.save({'vocab': word_to_id, 'embeddings': word_vectors}, path)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T05:55:36.900092Z","iopub.execute_input":"2024-03-27T05:55:36.900437Z","iopub.status.idle":"2024-03-27T05:55:57.073168Z","shell.execute_reply.started":"2024-03-27T05:55:36.900411Z","shell.execute_reply":"2024-03-27T05:55:57.072116Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(15000,)\n29073\n[ 2.38967954e+02 -1.98912430e+02 -3.18034814e+01  1.45922528e+01\n -1.10350391e+02 -2.24186937e+01 -6.72344197e+01  1.96190356e+02\n  8.77838273e+01  8.92731905e+01 -2.34019817e+02  2.10341626e+02\n  6.72282883e+01  9.56071442e+00 -4.52351878e+01 -3.25257918e+01\n  7.42925214e+01 -1.24371699e+02  1.38230324e+02  3.48929672e+01\n  4.42453295e+01 -7.72874522e+01 -7.89865973e+01  7.88647612e+01\n -1.28621270e+01 -6.60074836e-01  9.14563267e+01  2.92702964e+01\n  2.06097758e+00 -6.28239428e+00 -1.89858927e+01  8.84912919e+01\n  6.85639244e-01  1.50818066e+00  2.37830542e+01 -2.89029785e+00\n -1.73221690e-01 -4.72059864e+00 -1.70343266e+01  2.23423907e+01\n -3.33105607e+01  9.75157478e+00 -1.08365942e+00  1.98231739e+01\n -2.32809901e+01  2.17781157e+01  3.12441416e+00  7.73959452e-01\n  3.94997326e+00 -2.10507181e+01 -2.75932877e+00 -5.66769951e+00\n  2.40435963e+01 -9.95149374e+00 -4.31441802e+00 -1.23176569e+01\n  1.47304174e+01 -5.98003727e+00  4.83171853e+00  5.75124061e+00\n -9.80181563e+00 -3.09602882e+00 -9.52251513e+00  1.06935882e+01\n -6.06684749e+00 -2.20337015e+01 -1.80534325e-01  6.95460779e-01\n  8.25092970e+00 -4.57739104e+00 -2.16710537e+00 -4.71158825e-01\n  1.76526042e+00  1.30162828e+01  1.06612276e+00 -1.79879215e-01\n -2.15016878e+00 -4.94600578e+00  3.60156285e+00  2.06315880e+00\n -1.14192855e+00  1.37171367e+01  9.08618390e-01 -8.34199460e+00\n  1.46862459e+01 -6.69323207e+00 -2.26363269e+00  1.06047923e+01\n -5.98488728e+00  1.08857971e+01  1.81189849e+00 -2.89835487e+00\n  2.13267618e+00  1.12074085e+00 -5.89366443e+00 -4.98851455e+00\n  2.86743235e+00  1.91497005e-01  7.71136606e+00  8.26487614e+00\n  2.78342763e+00  1.58438887e+01 -7.17326876e+00  4.52912323e+00\n  5.51324105e+00 -8.44271303e-01  5.21384774e+00 -5.14658913e-01\n -4.90584524e+00 -9.14953797e+00  1.15072248e-02 -2.95642427e+00\n  1.76587089e+00 -4.00015018e+00  1.95988747e+00  2.28896338e+00\n  3.68812076e+00  1.16627803e+00  3.84288984e+00  1.72083789e+00\n -7.39545315e+00 -9.32647101e+00 -3.11318716e+00  9.03578964e+00\n -3.53584185e+00  1.59626703e+00 -5.39360151e+00 -5.51498512e+00\n -7.23761425e-01  1.71704242e+00 -5.28684435e+00  2.83111660e+00\n  2.75349163e+00  1.07211803e+00 -2.23243532e+00 -2.98621135e+00\n -1.21118356e+00  2.20895121e-01  2.81427519e-01  5.62945100e+00\n  5.41066469e+00  1.22008959e-01  3.12763470e+00  1.85304685e+00\n  1.73711336e+00 -7.04510947e-02 -2.84727556e+00 -5.39659178e+00\n -5.92764439e-01 -1.10348191e+00]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(word_vectors.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T05:55:57.074809Z","iopub.execute_input":"2024-03-27T05:55:57.075130Z","iopub.status.idle":"2024-03-27T05:55:57.079964Z","shell.execute_reply.started":"2024-03-27T05:55:57.075104Z","shell.execute_reply":"2024-03-27T05:55:57.079045Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(29073, 150)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load the saved word vectors\nmodel_path = '/kaggle/working/svd-word-vectors_15k.pt'\ncheckpoint = torch.load(model_path)\nvocab = checkpoint['vocab']\nembeddings = checkpoint['embeddings']\n\n# Define a mapping from words to indices\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nUNK_IDX = len(vocab)\n\n# Load the News Classification Dataset\ndf = pd.read_csv('/kaggle/input/train-csv/train.csv')\ndescriptions = df['Description'].iloc[0:15000]\nlabels = df['Class Index'].iloc[0:15000]\n\n# Tokenize the text and convert it into sequences of word indices\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    indices = [word_to_idx[token] if token in word_to_idx else word_to_idx[\"<UNK>\"] for token in tokens]\n    return indices\n\nsentence_indices = []\nfor desc in descriptions:\n    indices = preprocess(desc)\n    sentence_indices.append(indices)\n\n\n\n# Convert numpy arrays to tensors\nsentence_tensors = [torch.tensor(indices) for indices in sentence_indices]\n\n# Pad the sequences\npadded_sequences = pad_sequence(sentence_tensors, batch_first=True, padding_value=0)\n\n\n# Define a PyTorch Dataset for the News Classification Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        one_hot_target = np.zeros(4)\n        one_hot_target[self.y[idx]-1] = 1\n        #return np.array(self.X[idx]),  one_hot_target\n        return self.X[idx], torch.tensor(one_hot_target, dtype=torch.float32)\n\n\n# Create DataLoader objects for training and validation sets\n# Create DataLoader objects for training and validation sets\ntrain_dataset = NewsDataset(padded_sequences, labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n\n# Define an RNN model for classification\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings, dtype=torch.float32))\n        self.rnn = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.rnn(embedded)\n        logits = self.fc(output[:, -1, :])\n        return logits\n\n# Initialize the RNN classifier\ninput_size = embeddings.shape[1]\nhidden_size = 300\noutput_size = 4  # Number of classes\nrnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_classifier.parameters(), lr=0.001)\n\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    rnn_classifier.train()\n    for tokens, labels in train_loader:\n        optimizer.zero_grad()\n        # Convert tokens to long tensor\n        tokens = tokens.long()\n        # Forward pass\n        logits = rnn_classifier(tokens)\n        # Calculate loss\n        loss = criterion(logits, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n\n    # Print the loss every epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# Save the trained model\ntorch.save(rnn_classifier.state_dict(), 'rnn_classifier_15K.pt')\n\n\n\n#     # Validation loop\n#     rnn_classifier.eval()\n#     val_loss = 0.0\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for tokens, labels in val_loader:\n#             logits = rnn_classifier(tokens)\n#             val_loss += criterion(logits, labels).item()\n#             _, predicted = torch.max(logits, 1)\n#             correct += (predicted == labels).sum().item()\n#             total += labels.size(0)\n\n#     val_loss /= len(val_loader)\n#     accuracy = correct / total\n#     print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}, Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T05:56:19.012146Z","iopub.execute_input":"2024-03-27T05:56:19.012618Z","iopub.status.idle":"2024-03-27T08:45:13.255589Z","shell.execute_reply.started":"2024-03-27T05:56:19.012587Z","shell.execute_reply":"2024-03-27T08:45:13.254488Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.7233729362487793\nEpoch 2/10, Loss: 0.5456537008285522\nEpoch 3/10, Loss: 0.4666527807712555\nEpoch 4/10, Loss: 0.4247359335422516\nEpoch 5/10, Loss: 0.319564551115036\nEpoch 6/10, Loss: 0.30256354808807373\nEpoch 7/10, Loss: 0.357319712638855\nEpoch 8/10, Loss: 0.21016477048397064\nEpoch 9/10, Loss: 0.1832079142332077\nEpoch 10/10, Loss: 0.22311004996299744\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/anlp-3/test.csv')\ntest_descriptions = test_df['Description']\ntest_labels = test_df['Class Index']\n\n# Tokenize and preprocess the test descriptions\ntest_sentence_indices = []\nfor desc in test_descriptions:\n    indices = preprocess(desc)\n    test_sentence_indices.append(indices)\ntest_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\npadded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# Create a DataLoader for the test dataset\ntest_dataset = NewsDataset(padded_test_sequences, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n\n# Evaluate the model on the test dataset\nrnn_classifier.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        #print(logits.shape)\n        #print(labels.shape)\n        _, predicted = torch.max(logits, 1)\n        #print(predicted.shape)\n        total += labels.size(0)\n        correct += (predicted == labels.argmax(dim=1)).sum().item()\n\n# Calculate the accuracy\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T08:46:01.051831Z","iopub.execute_input":"2024-03-27T08:46:01.052106Z","iopub.status.idle":"2024-03-27T08:46:24.451284Z","shell.execute_reply.started":"2024-03-27T08:46:01.052080Z","shell.execute_reply":"2024-03-27T08:46:24.450218Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7772368421052631\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom scipy.sparse import coo_matrix\nfrom sklearn.decomposition import TruncatedSVD\nfrom collections import defaultdict\nimport string\nimport re\nimport torch\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/nlpass-2/train.csv')\n\n# Extract descriptions\ndescriptions = df['Description'].iloc[0:15000]\nprint(descriptions.shape)\n# Preprocessing function\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    \n    return tokens\n\n# Build co-occurrence matrix\ndef build_co_occurrence_matrix(descriptions, window_size=2):\n    word_to_id = {}\n    \n    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n    word_freq = defaultdict(int)\n    \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for token in tokens:\n            word_freq[token] += 1\n        \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for i, word in enumerate(tokens):\n            if word_freq[word] < 5:\n#                 word = '<UNK>'\n                tokens[i]=\"<UNK>\"\n            if word not in word_to_id:\n                word_id = len(word_to_id)\n                word_to_id[word] = word_id\n\n            start_index = max(0, i - window_size)\n            end_index = min(len(tokens), i + window_size + 1)\n            context_words = tokens[start_index:i] + tokens[i+1:end_index]\n            \n            for context_word in context_words:\n                if word_freq[context_word] < 5:\n                    context_word = '<UNK>'\n                if context_word not in word_to_id:\n                    word_id = len(word_to_id)\n                    word_to_id[context_word] = word_id\n                co_occurrence_matrix[word_to_id[word]][word_to_id[context_word]] += 1\n    print(len(word_to_id))\n\n    return co_occurrence_matrix, word_to_id\n\n# Convert co-occurrence matrix to a sparse matrix\ndef co_occurrence_to_sparse_matrix(co_occurrence_matrix):\n    rows = []\n    cols = []\n    data = []\n    for i, row in co_occurrence_matrix.items():\n        for j, value in row.items():\n            rows.append(i)\n            cols.append(j)\n            data.append(value)\n    return coo_matrix((data, (rows, cols)))\n\n# Apply Singular Value Decomposition (SVD)\ndef apply_svd(matrix, k=150):\n    svd = TruncatedSVD(n_components=k, random_state=42)\n    word_vectors = svd.fit_transform(matrix)\n    return word_vectors\n\n# Save word vectors\ndef save_word_vectors(word_vectors, word_to_id, filename):\n    with open(filename, 'w') as f:\n        for word, word_id in word_to_id.items():\n            vector_str = ' '.join(map(str, word_vectors[word_id]))\n            f.write(f'{word} {vector_str}\\n')\n\n# Build co-occurrence matrix\nco_occurrence_matrix, word_to_id = build_co_occurrence_matrix(descriptions)\n\n# Convert co-occurrence matrix to sparse matrix\nsparse_matrix = co_occurrence_to_sparse_matrix(co_occurrence_matrix)\n\n# Apply SVD\nword_vectors = apply_svd(sparse_matrix)\n\n# Save word vectors\n\nprint(word_vectors[0])\n\nmodel_save_name = 'svd-word-vectors_15kw2.pt'\npath = F\"/kaggle/working/{model_save_name}\"\ntorch.save({'vocab': word_to_id, 'embeddings': word_vectors}, path)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T05:38:29.995953Z","iopub.execute_input":"2024-03-31T05:38:29.996324Z","iopub.status.idle":"2024-03-31T05:38:51.250034Z","shell.execute_reply.started":"2024-03-31T05:38:29.996298Z","shell.execute_reply":"2024-03-31T05:38:51.249035Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(15000,)\n29073\n[ 3.93886366e+02 -1.70733257e+02  9.05897412e+01 -5.51097719e+01\n -3.58735590e+01  5.44273571e+00  1.02247908e+02  6.61618249e+01\n  6.36613967e+01  6.41885892e+00  4.96967481e+02 -9.07323541e+00\n -1.09884100e+01 -1.40052230e+02  7.28474184e+01  9.00364686e+01\n -5.59494064e+01  4.93262674e+01  1.21829103e+02  1.88362707e+02\n  1.86142119e+02  1.17878562e+02  6.88133611e+01 -1.17212610e+02\n  3.44770725e+00  4.64432692e+01  1.29562742e+01 -1.60241039e+01\n  5.28843521e+01 -1.70908111e+01  3.71664163e+01 -2.83387623e+01\n  4.47715203e+00 -1.04605707e+01 -3.26144481e+01 -1.86034168e+01\n  6.95032913e+00 -3.05699874e+00  9.85825593e+00 -4.11166772e+01\n  1.85408510e+01 -3.70455178e+01 -2.88417639e+01  8.35867757e+00\n  1.82581430e+00  1.89825237e+01 -1.24967080e+01 -3.72607007e+01\n -7.89206094e+00  2.12317912e+00  5.09469703e-01  6.34890564e+00\n -1.22205287e+01  6.72082740e-01 -2.69369389e+01 -2.36481922e+01\n  3.21297374e+00 -1.14368631e+01 -1.53480587e+01  8.82453894e+00\n  1.45031934e+01  1.62817365e+01 -7.34019898e+00  1.37527202e+01\n  2.13860604e+01  1.58883793e+01  3.80566472e+01 -1.02358324e+01\n  2.45243502e+01 -6.20426085e+00  7.07250074e+00 -3.92226701e+00\n -2.89655497e+00  5.63107386e+00  3.60212266e+00  2.42337121e+00\n  7.34026569e+00  1.70310411e+01  2.01510635e+00 -3.00863251e-01\n -4.10582452e+00 -2.56801435e+00  2.01977545e+00  1.32406515e+00\n  9.44357094e+00  5.11045829e+00 -9.68059673e+00  6.85325327e+00\n -7.66323901e-02  4.58547494e-01 -8.64973332e+00  6.93101503e+00\n -2.41799446e+00  1.12578136e+00  2.61665901e+00 -1.00906334e+01\n -7.86404640e-01 -4.93455994e+00  2.72345293e-01 -8.00251320e-01\n -2.10485285e+00  3.88667695e-01  7.13840179e+00 -1.02959658e+00\n  7.36153775e-01 -1.94238647e+00  3.14040920e-01 -2.00351691e-01\n  3.10230142e-01  1.57001658e+00  1.36475056e+00  3.27987980e-01\n  3.42831063e+00 -5.21351847e-01 -9.59600841e-01  2.61182404e+00\n  3.33985691e+00  3.45394916e+00 -3.75824515e-02 -1.65313635e+00\n -1.72417134e+00  3.48894358e+00  2.52008079e-01 -3.99630580e+00\n -9.06506498e-01  1.65716424e+00  1.25333809e+00 -1.06046741e+00\n  3.23930289e+00 -2.95628761e+00  1.47215037e+00  3.81046522e-01\n -2.08867617e+00 -8.15414480e-01 -2.20063043e+00 -2.15608533e+00\n -7.64299269e-01 -1.12733977e+00  4.72144109e+00  3.48664166e+00\n -3.78136536e+00 -4.29683394e+00 -3.05407137e-02  3.20537057e+00\n -3.50397869e-01 -1.68277729e-01  5.01719664e-01 -1.61243651e+00\n -3.55652503e+00 -4.15988365e+00]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load the saved word vectors\nmodel_path = '/kaggle/working/svd-word-vectors_15kw2.pt'\ncheckpoint = torch.load(model_path)\nvocab = checkpoint['vocab']\nembeddings = checkpoint['embeddings']\n\n# Define a mapping from words to indices\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nUNK_IDX = len(vocab)\n\n# Load the News Classification Dataset\ndf = pd.read_csv('/kaggle/input/nlpass-2/train.csv')\ndescriptions = df['Description'].iloc[0:15000]\nlabels = df['Class Index'].iloc[0:15000]\n\n# Tokenize the text and convert it into sequences of word indices\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    indices = [word_to_idx[token] if token in word_to_idx else word_to_idx[\"<UNK>\"] for token in tokens]\n    return indices\n\nsentence_indices = []\nfor desc in descriptions:\n    indices = preprocess(desc)\n    sentence_indices.append(indices)\n\n\n\n# Convert numpy arrays to tensors\nsentence_tensors = [torch.tensor(indices) for indices in sentence_indices]\n\n# Pad the sequences\npadded_sequences = pad_sequence(sentence_tensors, batch_first=True, padding_value=0)\n\n\n# Define a PyTorch Dataset for the News Classification Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        one_hot_target = np.zeros(4)\n        one_hot_target[self.y[idx]-1] = 1\n        #return np.array(self.X[idx]),  one_hot_target\n        return self.X[idx], torch.tensor(one_hot_target, dtype=torch.float32)\n\n\n# Create DataLoader objects for training and validation sets\n# Create DataLoader objects for training and validation sets\ntrain_dataset = NewsDataset(padded_sequences, labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n\n# Define an RNN model for classification\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings, dtype=torch.float32))\n        self.rnn = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.rnn(embedded)\n        logits = self.fc(output[:, -1, :])\n        return logits\n\n# Initialize the RNN classifier\ninput_size = embeddings.shape[1]\nhidden_size = 300\noutput_size = 4  # Number of classes\nrnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_classifier.parameters(), lr=0.001)\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    rnn_classifier.train()\n    for tokens, labels in train_loader:\n        optimizer.zero_grad()\n        # Convert tokens to long tensor\n        tokens = tokens.long()\n        # Forward pass\n        logits = rnn_classifier(tokens)\n        # Calculate loss\n        loss = criterion(logits, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n\n    # Print the loss every epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# Save the trained model\ntorch.save(rnn_classifier.state_dict(), 'rnn_classifier_15K_w2.pt')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T05:39:32.665519Z","iopub.execute_input":"2024-03-31T05:39:32.665903Z","iopub.status.idle":"2024-03-31T07:33:58.718470Z","shell.execute_reply.started":"2024-03-31T05:39:32.665873Z","shell.execute_reply":"2024-03-31T07:33:58.717453Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.6454418897628784\nEpoch 2/5, Loss: 0.4883682131767273\nEpoch 3/5, Loss: 0.37086960673332214\nEpoch 4/5, Loss: 0.4094800055027008\nEpoch 5/5, Loss: 0.3027333915233612\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the test dataset\ntest_df =pd.read_csv('/kaggle/input/nlpass-2/test.csv')\ntest_descriptions = test_df['Description']\ntest_labels = test_df['Class Index']\n\n# Tokenize and preprocess the test descriptions\ntest_sentence_indices = []\nfor desc in test_descriptions:\n    indices = preprocess(desc)\n    test_sentence_indices.append(indices)\ntest_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\npadded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# Create a DataLoader for the test dataset\ntest_dataset = NewsDataset(padded_test_sequences, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n\n# Evaluate the model on the test dataset\nrnn_classifier.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        #print(logits.shape)\n        #print(labels.shape)\n        _, predicted = torch.max(logits, 1)\n        #print(predicted.shape)\n        total += labels.size(0)\n        correct += (predicted == labels.argmax(dim=1)).sum().item()\n\n# Calculate the accuracy\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T07:33:58.720702Z","iopub.execute_input":"2024-03-31T07:33:58.721327Z","iopub.status.idle":"2024-03-31T07:40:13.076563Z","shell.execute_reply.started":"2024-03-31T07:33:58.721285Z","shell.execute_reply":"2024-03-31T07:40:13.075469Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8142416666666666\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n# # Load the trained RNN classifier\n# rnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n# rnn_classifier.load_state_dict(torch.load('rnn_classifier_15K.pt'))\n# rnn_classifier.eval()\n\n# # Tokenize and preprocess the test descriptions\n# test_sentence_indices = []\n# for desc in test_descriptions:\n#     indices = preprocess(desc)\n#     test_sentence_indices.append(indices)\n# test_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\n# padded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# # Create a DataLoader for the test dataset\n# test_dataset = NewsDataset(padded_test_sequences, test_labels)\n# test_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n\n# Make predictions on the test set\ntest_predictions = []\ntest_targets = []\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        _, predicted = torch.max(logits, 1)\n        test_predictions.extend(predicted.tolist())\n        test_targets.extend(labels.argmax(dim=1).tolist())\n\n# Compute performance metrics for the test set\ntest_accuracy = accuracy_score(test_targets, test_predictions)\ntest_f1 = f1_score(test_targets, test_predictions, average='weighted')\ntest_precision = precision_score(test_targets, test_predictions, average='weighted')\ntest_recall = recall_score(test_targets, test_predictions, average='weighted')\ntest_conf_matrix = confusion_matrix(test_targets, test_predictions)\n\n# Print the performance metrics for the test set\nprint(\"\\nTest Set:\")\nprint(f\"Accuracy: {test_accuracy}\")\nprint(f\"F1 Score: {test_f1}\")\nprint(f\"Precision: {test_precision}\")\nprint(f\"Recall: {test_recall}\")\nprint(\"Confusion Matrix:\")\nprint(test_conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T07:40:13.078142Z","iopub.execute_input":"2024-03-31T07:40:13.078490Z","iopub.status.idle":"2024-03-31T07:45:32.792638Z","shell.execute_reply.started":"2024-03-31T07:40:13.078461Z","shell.execute_reply":"2024-03-31T07:45:32.791399Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nTest Set:\nAccuracy: 0.8142416666666666\nF1 Score: 0.8152065635158733\nPrecision: 0.8199399851780164\nRecall: 0.8142416666666666\nConfusion Matrix:\n[[24575  1173  1692  2560]\n [ 1591 25721   750  1938]\n [ 2255   372 22169  5204]\n [ 1515   611  2630 25244]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom scipy.sparse import coo_matrix\nfrom sklearn.decomposition import TruncatedSVD\nfrom collections import defaultdict\nimport string\nimport re\nimport torch\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/nlpass-2/train.csv')\n\n# Extract descriptions\ndescriptions = df['Description'].iloc[0:15000]\nprint(descriptions.shape)\n# Preprocessing function\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    \n    return tokens\n\n# Build co-occurrence matrix\ndef build_co_occurrence_matrix(descriptions, window_size=3):\n    word_to_id = {}\n    \n    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n    word_freq = defaultdict(int)\n    \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for token in tokens:\n            word_freq[token] += 1\n        \n    for desc in descriptions:\n        tokens = preprocess(desc)\n        for i, word in enumerate(tokens):\n            if word_freq[word] < 5:\n#                 word = '<UNK>'\n                tokens[i]=\"<UNK>\"\n            if word not in word_to_id:\n                word_id = len(word_to_id)\n                word_to_id[word] = word_id\n\n            start_index = max(0, i - window_size)\n            end_index = min(len(tokens), i + window_size + 1)\n            context_words = tokens[start_index:i] + tokens[i+1:end_index]\n            \n            for context_word in context_words:\n                if word_freq[context_word] < 5:\n                    context_word = '<UNK>'\n                if context_word not in word_to_id:\n                    word_id = len(word_to_id)\n                    word_to_id[context_word] = word_id\n                co_occurrence_matrix[word_to_id[word]][word_to_id[context_word]] += 1\n    print(len(word_to_id))\n\n    return co_occurrence_matrix, word_to_id\n\n# Convert co-occurrence matrix to a sparse matrix\ndef co_occurrence_to_sparse_matrix(co_occurrence_matrix):\n    rows = []\n    cols = []\n    data = []\n    for i, row in co_occurrence_matrix.items():\n        for j, value in row.items():\n            rows.append(i)\n            cols.append(j)\n            data.append(value)\n    return coo_matrix((data, (rows, cols)))\n\n# Apply Singular Value Decomposition (SVD)\ndef apply_svd(matrix, k=150):\n    svd = TruncatedSVD(n_components=k, random_state=42)\n    word_vectors = svd.fit_transform(matrix)\n    return word_vectors\n\n# Save word vectors\ndef save_word_vectors(word_vectors, word_to_id, filename):\n    with open(filename, 'w') as f:\n        for word, word_id in word_to_id.items():\n            vector_str = ' '.join(map(str, word_vectors[word_id]))\n            f.write(f'{word} {vector_str}\\n')\n\n# Build co-occurrence matrix\nco_occurrence_matrix, word_to_id = build_co_occurrence_matrix(descriptions)\n\n# Convert co-occurrence matrix to sparse matrix\nsparse_matrix = co_occurrence_to_sparse_matrix(co_occurrence_matrix)\n\n# Apply SVD\nword_vectors = apply_svd(sparse_matrix)\n\n# Save word vectors\n\nprint(word_vectors[0])\n\nmodel_save_name = 'svd-word-vectors_15kw3.pt'\npath = F\"/kaggle/working/{model_save_name}\"\ntorch.save({'vocab': word_to_id, 'embeddings': word_vectors}, path)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:27:09.882349Z","iopub.execute_input":"2024-03-31T11:27:09.883202Z","iopub.status.idle":"2024-03-31T11:27:32.351181Z","shell.execute_reply.started":"2024-03-31T11:27:09.883168Z","shell.execute_reply":"2024-03-31T11:27:32.350195Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(15000,)\n29073\n[ 5.91823576e+02 -1.16666133e+02  5.44429054e+01 -6.88672119e+01\n  6.85894615e+01 -5.94948311e+01 -5.85716028e+00  8.22193952e+01\n  7.25098084e+01 -8.68838576e+01  5.40413211e+02 -7.02963866e+01\n  8.59588461e+00 -1.13214218e+02  8.21946329e+01 -4.22203124e+01\n -5.94127414e+01 -1.30626022e+02  9.32800670e+01  8.28092751e+01\n  8.38780340e+01 -5.31308609e+00 -1.73622329e+02  2.75212697e+02\n -3.44060573e+01 -4.84584971e+01  5.03289962e+01  1.01372706e+01\n  4.47421671e+01 -8.09111460e+00  7.25447396e-01  7.14467585e+00\n  1.63696954e+01  4.96947859e+00 -3.65177778e+01  3.70724313e-01\n -1.85202007e-01 -4.68612606e+00  3.57196787e+01  1.38751648e+01\n  3.71869761e+00  4.48563479e+01  2.23759275e+01  5.81129454e+00\n -5.23062959e+00  1.14618726e+01 -2.71524641e+01 -9.99529027e+00\n  6.17277296e-02 -5.36274526e+00 -2.42086812e+00 -6.99425861e+00\n -7.29734412e+00 -6.55322607e+00 -3.54655015e+00 -1.76277198e+01\n -1.95209985e+01  1.71077401e+01  5.74908831e+00  2.32467872e+01\n -2.30163662e+01  3.04727830e+01  2.36460795e+01  2.18230943e+00\n  2.44115180e+01 -1.40174634e-01 -1.86239944e+01  1.24588369e+01\n  1.83534716e+01 -3.10431924e+00  4.58636014e+00  2.54830154e+01\n  7.53884132e+00  7.18917135e+00 -1.57678429e+01  2.82614164e+00\n  2.67841939e+00  1.37158141e+00 -1.24567669e+01 -3.92345234e+00\n -4.79284664e+00  1.20306416e+01  4.54257634e+00 -7.03333775e+00\n  8.74803784e+00 -5.25626980e+00 -8.81748556e-01 -1.08081048e+01\n -8.07791741e+00 -2.96562991e+00 -7.64525350e+00 -3.24776598e+00\n -1.62824250e+00 -1.37556645e+00  1.36457498e+00 -3.03107980e+00\n -2.01473975e+00  2.86327856e-01 -2.67362469e+00  2.54785593e+00\n  4.03921612e+00  1.60239160e+00  7.56882149e-01  1.51995040e-01\n -5.51920262e+00 -1.12785394e+00 -4.75960010e+00  1.20893878e+00\n  1.94814152e+00 -1.60172364e+00  3.32377711e+00  3.34165445e+00\n  4.59680443e+00  5.24459443e-01  4.28798456e+00  1.81234081e+00\n -4.93470159e+00 -3.96827604e+00  2.83198097e+00  2.20785308e+00\n  3.10464531e+00  6.70585405e-01 -2.79683320e+00  3.88067874e-01\n  5.98025052e-01  1.06275812e+00 -6.38028504e-01 -1.02780039e+00\n  4.26947590e+00 -2.35244850e+00  4.13432046e+00  5.10492665e+00\n -3.14478970e+00  5.02597749e+00  2.04062392e+00 -1.19993700e+00\n -1.65929131e+00  2.99912395e-01  2.99878499e+00 -2.05067010e+00\n -4.14832420e+00 -2.33846183e-01 -2.43667032e+00 -6.21498232e+00\n  2.79409132e-01 -6.75050321e-01  1.43735120e+00  4.39078522e+00\n -1.73167877e+00 -7.60095530e-01]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load the saved word vectors\nmodel_path = '/kaggle/working/svd-word-vectors_15kw3.pt'\ncheckpoint = torch.load(model_path)\nvocab = checkpoint['vocab']\nembeddings = checkpoint['embeddings']\n\n# Define a mapping from words to indices\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nUNK_IDX = len(vocab)\n\n# Load the News Classification Dataset\ndf = pd.read_csv('/kaggle/input/nlpass-2/train.csv')\ndescriptions = df['Description'].iloc[0:15000]\nlabels = df['Class Index'].iloc[0:15000]\n\n# Tokenize the text and convert it into sequences of word indices\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    indices = [word_to_idx[token] if token in word_to_idx else word_to_idx[\"<UNK>\"] for token in tokens]\n    return indices\n\nsentence_indices = []\nfor desc in descriptions:\n    indices = preprocess(desc)\n    sentence_indices.append(indices)\n\n\n\n# Convert numpy arrays to tensors\nsentence_tensors = [torch.tensor(indices) for indices in sentence_indices]\n\n# Pad the sequences\npadded_sequences = pad_sequence(sentence_tensors, batch_first=True, padding_value=0)\n\n\n# Define a PyTorch Dataset for the News Classification Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        one_hot_target = np.zeros(4)\n        one_hot_target[self.y[idx]-1] = 1\n        #return np.array(self.X[idx]),  one_hot_target\n        return self.X[idx], torch.tensor(one_hot_target, dtype=torch.float32)\n\n\n# Create DataLoader objects for training and validation sets\n# Create DataLoader objects for training and validation sets\ntrain_dataset = NewsDataset(padded_sequences, labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n\n# Define an RNN model for classification\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings, dtype=torch.float32))\n        self.rnn = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.rnn(embedded)\n        logits = self.fc(output[:, -1, :])\n        return logits\n\n# Initialize the RNN classifier\ninput_size = embeddings.shape[1]\nhidden_size = 400\noutput_size = 4  # Number of classes\nrnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_classifier.parameters(), lr=0.001)\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    rnn_classifier.train()\n    for tokens, labels in train_loader:\n        optimizer.zero_grad()\n        # Convert tokens to long tensor\n        tokens = tokens.long()\n        # Forward pass\n        logits = rnn_classifier(tokens)\n        # Calculate loss\n        loss = criterion(logits, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n\n    # Print the loss every epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# Save the trained model\ntorch.save(rnn_classifier.state_dict(), 'rnn_classifier_15K_w3.pt')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:27:32.353044Z","iopub.execute_input":"2024-03-31T11:27:32.353364Z","iopub.status.idle":"2024-03-31T14:16:26.500720Z","shell.execute_reply.started":"2024-03-31T11:27:32.353322Z","shell.execute_reply":"2024-03-31T14:16:26.499717Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.5013253688812256\nEpoch 2/5, Loss: 0.4399815797805786\nEpoch 3/5, Loss: 0.3251735270023346\nEpoch 4/5, Loss: 0.35184961557388306\nEpoch 5/5, Loss: 0.31132203340530396\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the test dataset\ntest_df =pd.read_csv('/kaggle/input/nlpass-2/test.csv')\ntest_descriptions = test_df['Description']\ntest_labels = test_df['Class Index']\n\n# Tokenize and preprocess the test descriptions\ntest_sentence_indices = []\nfor desc in test_descriptions:\n    indices = preprocess(desc)\n    test_sentence_indices.append(indices)\ntest_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\npadded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# Create a DataLoader for the test dataset\ntest_dataset = NewsDataset(padded_test_sequences, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n# Evaluate the model on the test dataset\nrnn_classifier.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        #print(logits.shape)\n        #print(labels.shape)\n        _, predicted = torch.max(logits, 1)\n        #print(predicted.shape)\n        total += labels.size(0)\n        correct += (predicted == labels.argmax(dim=1)).sum().item()\n\n# Calculate the accuracy\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:25:37.388497Z","iopub.execute_input":"2024-03-31T14:25:37.388787Z","iopub.status.idle":"2024-03-31T14:34:44.102279Z","shell.execute_reply.started":"2024-03-31T14:25:37.388764Z","shell.execute_reply":"2024-03-31T14:34:44.101255Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8153166666666667\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Placeholder for averaged embeddings of test descriptions\naveraged_embeddings = []\n\n# Evaluate the model on the test dataset to obtain token embeddings\nrnn_classifier.eval()\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        embeddings = rnn_classifier.embedding(tokens)\n        averaged_embedding = embeddings.mean(dim=1)  # Average along the sequence length dimension\n        averaged_embeddings.append(averaged_embedding)\n\n# Concatenate all averaged embeddings\nall_averaged_embeddings = torch.cat(averaged_embeddings, dim=0)\n\n# Calculate cosine similarity between each pair of embeddings\ncosine_similarities = cosine_similarity(all_averaged_embeddings, all_averaged_embeddings)\n\n# Print cosine similarities\nprint(\"Cosine Similarities:\")\nprint(cosine_similarities[0].shape)\nprint(cosine_similarities[0])\nprint(cosine_similarities)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T18:08:19.990353Z","iopub.execute_input":"2024-03-31T18:08:19.991425Z","iopub.status.idle":"2024-03-31T18:08:20.665414Z","shell.execute_reply.started":"2024-03-31T18:08:19.991376Z","shell.execute_reply":"2024-03-31T18:08:20.664280Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Cosine Similarities:\n(7600,)\n[1.0000002  0.94731545 0.90162927 ... 0.9793161  0.9754841  0.97433066]\n[[1.0000002  0.94731545 0.90162927 ... 0.9793161  0.9754841  0.97433066]\n [0.94731545 1.0000004  0.9893156  ... 0.9910423  0.9926687  0.99200624]\n [0.90162927 0.9893156  0.99999994 ... 0.96278197 0.9736912  0.9749968 ]\n ...\n [0.9793161  0.9910423  0.96278197 ... 1.         0.99620664 0.99466974]\n [0.9754841  0.9926687  0.9736912  ... 0.99620664 1.0000001  0.9996457 ]\n [0.97433066 0.99200624 0.9749968  ... 0.99466974 0.9996457  1.        ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n\n\n# Make predictions on the test set\ntest_predictions = []\ntest_targets = []\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        _, predicted = torch.max(logits, 1)\n        test_predictions.extend(predicted.tolist())\n        test_targets.extend(labels.argmax(dim=1).tolist())\n\n# Compute performance metrics for the test set\ntest_accuracy = accuracy_score(test_targets, test_predictions)\ntest_f1 = f1_score(test_targets, test_predictions, average='weighted')\ntest_precision = precision_score(test_targets, test_predictions, average='weighted')\ntest_recall = recall_score(test_targets, test_predictions, average='weighted')\ntest_conf_matrix = confusion_matrix(test_targets, test_predictions)\n\n# Print the performance metrics for the test set\nprint(\"\\nTest Set:\")\nprint(f\"Accuracy: {test_accuracy}\")\nprint(f\"F1 Score: {test_f1}\")\nprint(f\"Precision: {test_precision}\")\nprint(f\"Recall: {test_recall}\")\nprint(\"Confusion Matrix:\")\nprint(test_conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T18:08:27.580849Z","iopub.execute_input":"2024-03-31T18:08:27.581575Z","iopub.status.idle":"2024-03-31T18:08:55.026823Z","shell.execute_reply.started":"2024-03-31T18:08:27.581542Z","shell.execute_reply":"2024-03-31T18:08:55.025786Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nTest Set:\nAccuracy: 0.8153947368421053\nF1 Score: 0.8145327268165049\nPrecision: 0.816958838717576\nRecall: 0.8153947368421053\nConfusion Matrix:\n[[1547  103  112  138]\n [  87 1751   29   33]\n [ 111   58 1345  386]\n [  98   82  166 1554]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nimport pandas as pd\n\n# Make predictions on the test set\ntest_predictions = []\ntest_targets = []\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        _, predicted = torch.max(logits, 1)\n        test_predictions.extend(predicted.tolist())\n        test_targets.extend(labels.argmax(dim=1).tolist())\n\n# Compute performance metrics for the test set\ntest_accuracy = accuracy_score(test_targets, test_predictions)\ntest_f1 = f1_score(test_targets, test_predictions, average='weighted')\ntest_precision = precision_score(test_targets, test_predictions, average='weighted')\ntest_recall = recall_score(test_targets, test_predictions, average='weighted')\ntest_conf_matrix = confusion_matrix(test_targets, test_predictions)\n\n# Create a DataFrame for the confusion matrix with labels for the axes\nconf_matrix_df = pd.DataFrame(test_conf_matrix, \n                               index=[f\"Actual {i}\" for i in range(output_size)], \n                               columns=[f\"Predicted {i}\" for i in range(output_size)])\n\n# Print the performance metrics and the labeled confusion matrix for the test set\nprint(\"\\nTest Set:\")\nprint(f\"Accuracy: {test_accuracy}\")\nprint(f\"F1 Score: {test_f1}\")\nprint(f\"Precision: {test_precision}\")\nprint(f\"Recall: {test_recall}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T18:15:09.125205Z","iopub.execute_input":"2024-03-31T18:15:09.125662Z","iopub.status.idle":"2024-03-31T18:15:37.049827Z","shell.execute_reply.started":"2024-03-31T18:15:09.125629Z","shell.execute_reply":"2024-03-31T18:15:37.048747Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nTest Set:\nAccuracy: 0.8153947368421053\nF1 Score: 0.8145327268165049\nPrecision: 0.816958838717576\nRecall: 0.8153947368421053\nConfusion Matrix:\n          Predicted 0  Predicted 1  Predicted 2  Predicted 3\nActual 0         1547          103          112          138\nActual 1           87         1751           29           33\nActual 2          111           58         1345          386\nActual 3           98           82          166         1554\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Create a DataFrame for the confusion matrix with labels for the axes\nconf_matrix_df = pd.DataFrame(test_conf_matrix, \n                               index=[f\"Actual {i}\" for i in range(output_size)], \n                               columns=[f\"Predicted {i}\" for i in range(output_size)])\n\n# Create a figure and axis\nplt.figure(figsize=(8, 6))\nax = plt.gca()\n\n# Plot the confusion matrix as an image\ncmap = sns.color_palette(\"viridis\", as_cmap=True)\nsns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap=cmap, cbar=False, ax=ax)\n\n# Set the axis labels\nax.set_xlabel('Predicted', fontsize=14, color='blue')\nax.set_ylabel('Actual', fontsize=14, color='green')\n\n# Set the title\nax.set_title('Confusion Matrix', fontsize=16)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T18:16:38.522161Z","iopub.execute_input":"2024-03-31T18:16:38.523224Z","iopub.status.idle":"2024-03-31T18:16:38.968986Z","shell.execute_reply.started":"2024-03-31T18:16:38.523182Z","shell.execute_reply":"2024-03-31T18:16:38.968023Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load the saved word vectors\nmodel_path = '/kaggle/working/svd-word-vectors_15kw3.pt'\ncheckpoint = torch.load(model_path)\nvocab = checkpoint['vocab']\nembeddings = checkpoint['embeddings']\n\n# Define a mapping from words to indices\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nUNK_IDX = len(vocab)\n\n# Load the News Classification Dataset\ndf = pd.read_csv('/kaggle/input/nlpass-2/train.csv')\ndescriptions = df['Description'].iloc[0:15000]\nlabels = df['Class Index'].iloc[0:15000]\n\n# Tokenize the text and convert it into sequences of word indices\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    indices = [word_to_idx[token] if token in word_to_idx else word_to_idx[\"<UNK>\"] for token in tokens]\n    return indices\n\nsentence_indices = []\nfor desc in descriptions:\n    indices = preprocess(desc)\n    sentence_indices.append(indices)\n\n\n\n# Convert numpy arrays to tensors\nsentence_tensors = [torch.tensor(indices) for indices in sentence_indices]\n\n# Pad the sequences\npadded_sequences = pad_sequence(sentence_tensors, batch_first=True, padding_value=0)\n\n\n# Define a PyTorch Dataset for the News Classification Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        one_hot_target = np.zeros(4)\n        one_hot_target[self.y[idx]-1] = 1\n        #return np.array(self.X[idx]),  one_hot_target\n        return self.X[idx], torch.tensor(one_hot_target, dtype=torch.float32)\n\n\n# Create DataLoader objects for training and validation sets\n# Create DataLoader objects for training and validation sets\ntrain_dataset = NewsDataset(padded_sequences, labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n\n# Define an RNN model for classification\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings, dtype=torch.float32))\n        self.rnn = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.rnn(embedded)\n        logits = self.fc(output[:, -1, :])\n        return logits\n\n# Initialize the RNN classifier\ninput_size = embeddings.shape[1]\nhidden_size = 400\noutput_size = 4  # Number of classes\nrnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_classifier.parameters(), lr=0.001)\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    rnn_classifier.train()\n    for tokens, labels in train_loader:\n        optimizer.zero_grad()\n        # Convert tokens to long tensor\n        tokens = tokens.long()\n        # Forward pass\n        logits = rnn_classifier(tokens)\n        # Calculate loss\n        loss = criterion(logits, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n\n    # Print the loss every epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# Save the trained model\ntorch.save(rnn_classifier.state_dict(), 'rnn_classifier_15K_w3.pt')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T14:52:21.244139Z","iopub.execute_input":"2024-03-31T14:52:21.245147Z","iopub.status.idle":"2024-03-31T17:57:57.299200Z","shell.execute_reply.started":"2024-03-31T14:52:21.245110Z","shell.execute_reply":"2024-03-31T17:57:57.298035Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.6809596419334412\nEpoch 2/5, Loss: 0.42304062843322754\nEpoch 3/5, Loss: 0.4013839066028595\nEpoch 4/5, Loss: 0.42187297344207764\nEpoch 5/5, Loss: 0.3241225481033325\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the test dataset\ntest_df =pd.read_csv('/kaggle/input/nlpass-2/test.csv')\ntest_descriptions = test_df['Description']\ntest_labels = test_df['Class Index']\n\n# Tokenize and preprocess the test descriptions\ntest_sentence_indices = []\nfor desc in test_descriptions:\n    indices = preprocess(desc)\n    test_sentence_indices.append(indices)\ntest_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\npadded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# Create a DataLoader for the test dataset\ntest_dataset = NewsDataset(padded_test_sequences, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n# Evaluate the model on the test dataset\nrnn_classifier.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        #print(logits.shape)\n        #print(labels.shape)\n        _, predicted = torch.max(logits, 1)\n        #print(predicted.shape)\n        total += labels.size(0)\n        correct += (predicted == labels.argmax(dim=1)).sum().item()\n\n# Calculate the accuracy\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T18:04:41.636551Z","iopub.execute_input":"2024-03-31T18:04:41.637296Z","iopub.status.idle":"2024-03-31T18:05:12.430916Z","shell.execute_reply.started":"2024-03-31T18:04:41.637258Z","shell.execute_reply":"2024-03-31T18:05:12.429832Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8153947368421053\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}