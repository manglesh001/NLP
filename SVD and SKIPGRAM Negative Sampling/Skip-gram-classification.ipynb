{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7921409,"sourceType":"datasetVersion","datasetId":4655095},{"sourceId":7930308,"sourceType":"datasetVersion","datasetId":4661225},{"sourceId":7990130,"sourceType":"datasetVersion","datasetId":4703753},{"sourceId":7990681,"sourceType":"datasetVersion","datasetId":4704129}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, Counter\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\n# Assuming necessary libraries are imported, and descriptions are loaded as per your code\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/anlp-3/train.csv')\n\n# Extract descriptions\ndescriptions = df['Description'].iloc[0:15000]\n# Preprocess the descriptions\ndef preprocess(sentence):\n    tokens = word_tokenize(sentence.lower())\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    tokens = [word for word in tokens if word]\n    return tokens\n\n# Preprocess all descriptions\nclean_descriptions = [preprocess(desc) for desc in descriptions if isinstance(desc, str)]\n\n# Calculate word frequencies\nword_freq = Counter(word for sentence in clean_descriptions for word in sentence)\n\n# Build word to index mapping and replace infrequent words with 'UNK'\nword_to_index = {'<UNK>': 0}\nunk_count = 0  # Initialize a counter for 'UNK' frequencies\n\nfor word, freq in word_freq.items():\n    if freq >= 5:\n        word_to_index[word] = len(word_to_index)\n    else:\n        unk_count += freq  # Accumulate frequencies for 'UNK'\n\n# Now, update the 'UNK' count in 'word_freq'\nword_freq['<UNK>'] = unk_count\n\nindex_to_word = {index: word for word, index in word_to_index.items()}\n\n# Convert sentences to lists of indices\nsentences_as_indices = []\nfor sentence in clean_descriptions:\n    sentence_indices = [word_to_index.get(word, word_to_index['<UNK>']) for word in sentence]\n    sentences_as_indices.append(sentence_indices)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:19:39.703374Z","iopub.execute_input":"2024-03-31T13:19:39.703987Z","iopub.status.idle":"2024-03-31T13:19:47.592125Z","shell.execute_reply.started":"2024-03-31T13:19:39.703958Z","shell.execute_reply":"2024-03-31T13:19:47.591317Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(len(sentences_as_indices))","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:19:47.593530Z","iopub.execute_input":"2024-03-31T13:19:47.593837Z","iopub.status.idle":"2024-03-31T13:19:47.598599Z","shell.execute_reply.started":"2024-03-31T13:19:47.593813Z","shell.execute_reply":"2024-03-31T13:19:47.597763Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"15000\n","output_type":"stream"}]},{"cell_type":"code","source":"class SkipGramModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SkipGramModel, self).__init__()\n        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n\n\n    def forward(self, target_words, context_words):\n        target_embeds = self.target_embeddings(target_words)\n        context_embeds = self.context_embeddings(context_words)\n        # Dot product of target and context embeddings\n        dots = torch.einsum('be,bce->bc', target_embeds, context_embeds)\n        return dots\n  \n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:19:50.849785Z","iopub.execute_input":"2024-03-31T13:19:50.850607Z","iopub.status.idle":"2024-03-31T13:19:50.856577Z","shell.execute_reply.started":"2024-03-31T13:19:50.850578Z","shell.execute_reply":"2024-03-31T13:19:50.855609Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def generate_training_data(sentences, num_neg_samples, window_size, vocab_size):\n    targets, contexts, labels = [], [], []\n    pos_pairs = []\n    context_dict = {}\n    for sequence in sentences:\n        for idx, word_idx in enumerate(sequence):\n            context_dict[word_idx] = []\n            window_start = max(0, idx - window_size)\n            window_end = min(len(sequence), idx + window_size + 1)\n            for context_index in range(window_start, window_end):\n                if context_index != idx:\n                    pos_pairs.append((word_idx, sequence[context_index]))\n                    context_dict[word_idx].append(sequence[context_index])\n    for center, context in pos_pairs:\n        cntxt, lbl = [], []\n        targets.append(center)\n        cntxt.append(context)\n        lbl.append(1)\n        i = 0\n        while i < num_neg_samples:\n            negative_sample = np.random.randint(0, vocab_size)\n            if negative_sample != center and negative_sample not in context_dict[center]:\n                cntxt.append(negative_sample)\n                lbl.append(0)\n                i += 1\n        contexts.append(cntxt)\n        labels.append(lbl)\n    return targets, contexts, labels","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:19:59.074610Z","iopub.execute_input":"2024-03-31T13:19:59.074971Z","iopub.status.idle":"2024-03-31T13:19:59.084900Z","shell.execute_reply.started":"2024-03-31T13:19:59.074944Z","shell.execute_reply":"2024-03-31T13:19:59.083922Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class SkipGramDataset(Dataset):\n    def __init__(self, target_indices, context_indices, labels):\n#       self.target = target\n        self.target_indices = target_indices\n        self.context_indices = context_indices\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.target_indices)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.target_indices[idx]), torch.tensor(self.context_indices[idx]), torch.tensor(self.labels[idx])","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:20:16.909015Z","iopub.execute_input":"2024-03-31T13:20:16.909367Z","iopub.status.idle":"2024-03-31T13:20:16.915592Z","shell.execute_reply.started":"2024-03-31T13:20:16.909344Z","shell.execute_reply":"2024-03-31T13:20:16.914590Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nembedding_dim = 150  # Size of the embedding vector\nvocab_size = len(word_to_index)  # Total number of unique words in your vocabulary\nwindow_size=1\nlearning_rate = 0.001\nepochs = 10\nbatch_size = 300\nnum_neg_samples=2\n\n\n# Initialize dataset and data loader\ntarget_indices, context_indices, labels = generate_training_data(sentences_as_indices, num_neg_samples, window_size, vocab_size)\nprint(len(target_indices), len(context_indices), len(labels))\ndataset = SkipGramDataset(target_indices, context_indices, labels)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, loss, and optimizer\nmodel = SkipGramModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(epochs):\n    total_loss = 0\n    for target_word, context_word, label in data_loader:\n        optimizer.zero_grad()\n        scores = model(target_word, context_word)\n        label_float = label.float()\n        loss = criterion(scores, label_float)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'Epoch {epoch+1}, Loss: {total_loss}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_mean=(model.target_embeddings.weight + model.context_embeddings.weight)/2","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:28:41.417178Z","iopub.execute_input":"2024-03-31T13:28:41.417742Z","iopub.status.idle":"2024-03-31T13:28:41.429603Z","shell.execute_reply.started":"2024-03-31T13:28:41.417712Z","shell.execute_reply":"2024-03-31T13:28:41.428751Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model_save_name = 'skip-gram-word-vectors.pt'\npath = F\"/kaggle/working/{model_save_name}\"\ntorch.save({'vocab': word_to_index, 'embeddings': embeddings_mean}, path)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T13:28:41.430796Z","iopub.execute_input":"2024-03-31T13:28:41.431070Z","iopub.status.idle":"2024-03-31T13:28:41.459063Z","shell.execute_reply.started":"2024-03-31T13:28:41.431046Z","shell.execute_reply":"2024-03-31T13:28:41.458268Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load the saved word vectors\nmodel_path = '/kaggle/input/modelpath/skip-gram-word-vectors.pt'\ncheckpoint = torch.load(model_path)\nvocab = checkpoint['vocab']\nembeddings = checkpoint['embeddings']\n\n# Define a mapping from words to indices\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nUNK_IDX = len(vocab)\n\n# Load the News Classification Dataset\ndf = pd.read_csv('/kaggle/input/anlp-3/train.csv')\ndescriptions = df['Description'].iloc[0:15000]\nlabels = df['Class Index'].iloc[0:15000]\n\n# Tokenize the text and convert it into sequences of word indices\ndef preprocess(sentence):\n    # Tokenization\n    tokens = word_tokenize(sentence.lower())\n    # Remove special characters using regex\n    tokens = [re.sub(r'[^a-zA-Z0-9 ]', '', word) for word in tokens]\n    # Remove empty tokens\n    tokens = [word for word in tokens if word]\n    indices = [word_to_idx[token] if token in word_to_idx else word_to_idx['<UNK>'] for token in tokens]\n    return indices\n\nsentence_indices = []\nfor desc in descriptions:\n    indices = preprocess(desc)\n    sentence_indices.append(indices)\n\n\n\n# Convert numpy arrays to tensors\nsentence_tensors = [torch.tensor(indices) for indices in sentence_indices]\n\n# Pad the sequences\npadded_sequences = pad_sequence(sentence_tensors, batch_first=True, padding_value=0)\n\n\n# Define a PyTorch Dataset for the News Classification Dataset\nclass NewsDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        one_hot_target = np.zeros(4)\n        one_hot_target[self.y[idx]-1] = 1\n        #return np.array(self.X[idx]),  one_hot_target\n        return self.X[idx], torch.tensor(one_hot_target, dtype=torch.float32)\n\n\n# Create DataLoader objects for training and validation sets\n# Create DataLoader objects for training and validation sets\ntrain_dataset = NewsDataset(padded_sequences, labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n\n# Define an RNN model for classification\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNClassifier, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings, dtype=torch.float32))\n        self.rnn = nn.LSTM(input_size, hidden_size,num_layers=2, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.rnn(embedded)\n        logits = self.fc(output[:, -1, :])\n        return logits\n\n# Initialize the RNN classifier\ninput_size = embeddings.shape[1]\nhidden_size = 300\noutput_size = 4  # Number of classes\nrnn_classifier = RNNClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_classifier.parameters(), lr=0.001)\n\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    rnn_classifier.train()\n    for tokens, labels in train_loader:\n        optimizer.zero_grad()\n        # Convert tokens to long tensor\n        tokens = tokens.long()\n        # Forward pass\n        logits = rnn_classifier(tokens)\n        # Calculate loss\n        loss = criterion(logits, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n\n    # Print the loss every epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# Save the trained model\ntorch.save(rnn_classifier.state_dict(), 'rnn_classifier_skipgram_30K.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/anlp-3/test.csv')\ntest_descriptions = test_df['Description']\ntest_labels = test_df['Class Index']\n\n# Tokenize and preprocess the test descriptions\ntest_sentence_indices = []\nfor desc in test_descriptions:\n    indices = preprocess(desc)\n    test_sentence_indices.append(indices)\ntest_sentence_tensors = [torch.tensor(indices) for indices in test_sentence_indices]\npadded_test_sequences = pad_sequence(test_sentence_tensors, batch_first=True, padding_value=0)\n\n# Create a DataLoader for the test dataset\ntest_dataset = NewsDataset(padded_test_sequences, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=300, shuffle=False)\n\n# Evaluate the model on the test dataset\nrnn_classifier.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        #print(logits.shape)\n        #print(labels.shape)\n        _, predicted = torch.max(logits, 1)\n        #print(predicted.shape)\n        total += labels.size(0)\n        correct += (predicted == labels.argmax(dim=1)).sum().item()\n\n# Calculate the accuracy\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T09:13:12.650980Z","iopub.status.idle":"2024-03-29T09:13:12.651355Z","shell.execute_reply.started":"2024-03-29T09:13:12.651171Z","shell.execute_reply":"2024-03-29T09:13:12.651185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n\n# Train set predictions\ntrain_predictions = []\ntrain_targets = []\nwith torch.no_grad():\n    for tokens, labels in train_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        _, predicted = torch.max(logits, 1)\n        train_predictions.extend(predicted.tolist())\n        train_targets.extend(labels.argmax(dim=1).tolist())\n\n# Test set predictions\ntest_predictions = []\ntest_targets = []\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.long()\n        logits = rnn_classifier(tokens)\n        _, predicted = torch.max(logits, 1)\n        test_predictions.extend(predicted.tolist())\n        test_targets.extend(labels.argmax(dim=1).tolist())\n\n# Compute performance metrics\ntrain_accuracy = accuracy_score(train_targets, train_predictions)\ntrain_f1 = f1_score(train_targets, train_predictions, average='weighted')\ntrain_precision = precision_score(train_targets, train_predictions, average='weighted')\ntrain_recall = recall_score(train_targets, train_predictions, average='weighted')\ntrain_conf_matrix = confusion_matrix(train_targets, train_predictions)\n\ntest_accuracy = accuracy_score(test_targets, test_predictions)\ntest_f1 = f1_score(test_targets, test_predictions, average='weighted')\ntest_precision = precision_score(test_targets, test_predictions, average='weighted')\ntest_recall = recall_score(test_targets, test_predictions, average='weighted')\ntest_conf_matrix = confusion_matrix(test_targets, test_predictions)\n\n# Print the performance metrics\nprint(\"Train Set:\")\nprint(f\"Accuracy: {train_accuracy}\")\nprint(f\"F1 Score: {train_f1}\")\nprint(f\"Precision: {train_precision}\")\nprint(f\"Recall: {train_recall}\")\nprint(\"Confusion Matrix:\")\nprint(train_conf_matrix)\n\nprint(\"\\nTest Set:\")\nprint(f\"Accuracy: {test_accuracy}\")\nprint(f\"F1 Score: {test_f1}\")\nprint(f\"Precision: {test_precision}\")\nprint(f\"Recall: {test_recall}\")\nprint(\"Confusion Matrix:\")\nprint(test_conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-29T09:13:12.653008Z","iopub.status.idle":"2024-03-29T09:13:12.653352Z","shell.execute_reply.started":"2024-03-29T09:13:12.653188Z","shell.execute_reply":"2024-03-29T09:13:12.653201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}